{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Copy of Copy of FCN_FF_BP_class.ipynb","provenance":[{"file_id":"1vohaOEhRhLW3JA6Ypkz9uNfQstSPx5gM","timestamp":1654764877295},{"file_id":"1PjPDJYpIBF8FEdigCkv6of5wdM9RbTIg","timestamp":1654712701207}],"collapsed_sections":[],"authorship_tag":"ABX9TyPOZBl33whHVl1tN1Fc/1Mi"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"iAzvtKr2j4NZ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1654720730336,"user_tz":-180,"elapsed":104063,"user":{"displayName":"Yizhar Lavner","userId":"03277116809992444162"}},"outputId":"e0a71cda-5b64-424d-9ce1-2a03b6acf2e5"},"outputs":[{"output_type":"stream","name":"stdout","text":["Cost function J =  [[8.31771326]] in iteration 0 acc training set =  9.47075208913649\n","Cost function J =  [[0.68439992]] in iteration 50 acc training set =  98.60724233983287\n","Cost function J =  [[0.43479559]] in iteration 100 acc training set =  99.72144846796658\n","Cost function J =  [[0.37894043]] in iteration 150 acc training set =  100.0\n","Cost function J =  [[0.35091045]] in iteration 200 acc training set =  100.0\n","Cost function J =  [[0.33210792]] in iteration 250 acc training set =  100.0\n","Cost function J =  [[0.31798623]] in iteration 300 acc training set =  100.0\n","Cost function J =  [[0.3065693]] in iteration 350 acc training set =  100.0\n","Cost function J =  [[0.29704582]] in iteration 400 acc training set =  100.0\n","Cost function J =  [[0.28901587]] in iteration 450 acc training set =  100.0\n","Cost function J =  [[0.28221737]] in iteration 500 acc training set =  100.0\n","Cost function J =  [[0.27639684]] in iteration 550 acc training set =  100.0\n","Cost function J =  [[0.2712703]] in iteration 600 acc training set =  100.0\n","Cost function J =  [[0.2667735]] in iteration 650 acc training set =  100.0\n","Cost function J =  [[0.26266493]] in iteration 700 acc training set =  100.0\n","Cost function J =  [[0.25917615]] in iteration 750 acc training set =  100.0\n","Cost function J =  [[0.25607751]] in iteration 800 acc training set =  100.0\n","Cost function J =  [[0.253262]] in iteration 850 acc training set =  100.0\n","Cost function J =  [[0.25069738]] in iteration 900 acc training set =  100.0\n","Cost function J =  [[0.24836071]] in iteration 950 acc training set =  100.0\n","acc for X_test =  88.5952712100139\n"]}],"source":["# -*- coding: utf-8 -*-\n","\"\"\"\n","Created on Wed Jun  1 17:15:08 2022\n","\n","@author: YL\n","\"\"\"\n","\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import scipy.io as sio\n","import pandas as pd\n","import time\n","from sklearn import datasets\n","from sklearn.model_selection import train_test_split\n","\n","\n","\n","\n","def sigmoid(Z):\n","    \"\"\"\n","    Compute the sigmoid of Z\n","\n","    Arguments:\n","    Z - A scalar or numpy array of any size.\n","\n","    Return:\n","    A - sigmoid(Z)\n","    \"\"\"\n","    A = 1/(1+np.exp(-Z))\n","    \n","    return A\n","    \n","\n","\n","\n","def init_parameters(Lin, Lout):\n","    \"\"\"\n","    Init_parameters randomly initialize the parameters of a layer with Lin\n","    incoming inputs and Lout outputs \n","    Input arguments: \n","    Lin - the number of incoming inputs to the layer (not including the bias)\n","    Lout - the number of output connections \n","    Output arguments:\n","    Theta - the initial weight matrix, whose size is Lout x Lin+1 (the +1 is for the bias).    \n","    Usage: Theta = init_parameters(Lin, Lout)\n","    \n","    \"\"\"\n","    \n","    factor = np.sqrt(6/(Lin+Lout))\n","    Theta = np.zeros((Lout, Lin + 1))\n","    Theta = 2 * factor * (np.random.rand(Lout, Lin + 1) - 0.5)\n","    return Theta\n","    \n","    \n","    \n","\n","def ff_predict(Theta1, Theta2, X, y):\n","    \"\"\"\n","    ff_predict employs forward propagation on a 3 layer networks and\n","    determines the labels of  the inputs \n","    Input arguments\n","    Theta1 - matrix of parameters (weights)  between the input and the first hidden layer\n","    Theta2 - matrix of parameters (weights)  between the hidden layer and the output layer (or\n","          another hidden layer)\n","    X - input matrix\n","    y - input labels\n","    Output arguments:\n","    p - the predicted labels of the inputs\n","    Usage: p = ff_predict(Theta1, Theta2, X)\n","    \"\"\"\n","    m = X.shape[0]\n","    num_outputs = Theta2.shape[0]\n","    p = np.zeros((m,1))\n","    \n","    \n","    X_0 = np.ones((X.shape[0],1))\n","    X1 = np.concatenate((X_0, X), axis = 1)\n","    z2 = np.dot(X1, Theta1.T)\n","    a2 = sigmoid(z2)\n","    a2_0 = np.ones((a2.shape[0],1))\n","    a2 = np.concatenate((a2_0, a2), axis = 1)\n","    z3 = np.dot(a2, Theta2.T)\n","    a3 = sigmoid(z3)\n","    p = np.argmax(a3.T, axis = 0)\n","    p = p.reshape(p.shape[0],1)\n","    detectp = np.sum(p == y) / m * 100\n","    \n","    return p, detectp\n","\n","\n","def backprop(Theta1, Theta2, X, y, max_iter = 1000, alpha = 0.9, Lambda = 0):\n","    \"\"\"\n","    backprop - BackPropagation for training a neural network\n","    Input arguments\n","    Theta1 - matrix of parameters (weights)  between the input and the first \n","        hidden layer\n","    Theta2 - matrix of parameters (weights)  between the hidden layer and the \n","        output layer (or another hidden layer)\n","    X - input matrix\n","    y - labels of the input examples\n","    max_iter - maximum number of iterations (epochs).\n","    alpha - learning coefficient.\n","    Lambda - regularization coefficient.\n","    \n","    Output arguments\n","    J - the cost function\n","    Theta1 - updated weight matrix between the input and the first \n","        hidden layer\n","    Theta2 - updated weight matrix between the hidden layer and the output \n","        layer (or a second hidden layer)\n","    \n","    Usage:\n","    [J,Theta1,Theta2] = backprop(Theta1, Theta2, X,y,max_iter, alpha,Lambda)\n","    \"\"\"\n","    \n","    m = X.shape[0]\n","    num_outputs = Theta2.shape[0]\n","    delta3 = np.zeros((num_outputs, 1))\n","    ybin = np.zeros(delta3.shape)\n","    p = np.zeros((m, 1))\n","    \n","        p, acc = ff_predict(Theta1, Theta2, X, y)\n","        if np.mod(q, 50) == 0 :\n","            print('Cost function J = ', J, 'in iteration', q, \n","                      'acc training set = ', acc)\n","        time.sleep(0.05)\n","        # print('acc training set = ', acc)\n","        # time.sleep(0.005)\n","    return J, Theta1, Theta2\n","            \n"," \n","\n","digits = datasets.load_digits()\n","\n","# flatten the images\n","n_samples = len(digits.images)\n","data = digits.images.reshape((n_samples, -1))\n","\n","\n","# Split data into 50% train and 50% test subsets\n","X_train, X_test, y_train, y_test = train_test_split(\n","    data, digits.target, test_size=0.8, shuffle=False\n",")\n","\n","y_train = y_train.reshape((y_train.shape[0], 1))\n","y_test = y_test.reshape((y_test.shape[0], 1))\n","\n","L1 = X_train.shape[1]\n","num_outputs = np.unique(y_train).size\n","num_hidden = 120\n","Theta1 = init_parameters(L1, num_hidden)\n","Theta2 = init_parameters(num_hidden, num_outputs)\n","\n","[J,Theta1,Theta2] = backprop(Theta1, Theta2, X_train, y_train,1000, 0.5,1)\n","p, acc = ff_predict(Theta1, Theta2, X_test, y_test)\n","print('acc for X_test = ', acc)\n","\n","\n","\n","\n"]}]}